% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/skeleton-io.R
\name{read_segments}
\alias{read_segments}
\alias{read_segments2}
\title{Read skeletons for segments by extracting from the corresponding zip file(s)}
\usage{
read_segments(x, voxdims = c(32, 32, 40), ...)

read_segments2(x, voxdims = c(32, 32, 40), minfilesize = 80, ...)
}
\arguments{
\item{x}{A vector of segment ids (you might get these from a neuroglancer
scene)}

\item{voxdims}{The voxel dimensions in nm of the skeletonised data}

\item{...}{additional arguments passed to \code{\link[nat]{read.neurons}}}

\item{minfilesize}{The uncompressed size of the swc file must be >= this. A
cheap way to insist that we have >1 point.}
}
\value{
A \code{\link[nat]{neuronlist}} containing one
  \code{\link[nat]{neuron}} for each fragment
}
\description{
Read skeletons for segments by extracting from the corresponding zip file(s)

\code{read_segments2} is a reworked version of
  \code{read_segments} that reads skeletons straight from zip files to
  memory.
}
\details{
I would recommend \code{read_segments2} at this point.
  \code{read_segments} has the potential benefit of caching SWC files on disk
  rather than extracting every time. However there is a large slowdown on
  many filesystems as the number of extracted files enters the thousands -
  something that I have hit a few times. Furthermore \code{read_segments2}
  makes it easier to select fragment files \emph{before} extracting them.
}
\examples{
\dontrun{
n <- read_segments2(22427007374)
summary(n)
}
}
\seealso{
\code{\link[nat]{read.neurons}}
}
