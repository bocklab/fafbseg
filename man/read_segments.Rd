% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/skeleton-io.R
\name{read_segments}
\alias{read_segments}
\alias{read_segments2}
\title{Read skeletons for segments by extracting from the corresponding zip file(s)}
\usage{
read_segments(x, voxdims = c(32, 32, 40), ...)

read_segments2(x, voxdims = c(32, 32, 40), minfilesize = 80,
  datafrac = NULL, coordsonly = FALSE, ...)
}
\arguments{
\item{x}{A vector of segment ids or any Neuroglancer scene specification that
includes segments ids (see examples and \code{\link{ngl_segments}} for
details).}

\item{voxdims}{The voxel dimensions in nm of the skeletonised data}

\item{...}{additional arguments passed to \code{\link[nat]{read.neurons}}}

\item{minfilesize}{The uncompressed size of the swc file must be >= this. A
cheap way to insist that we have >1 point.}

\item{datafrac}{Fraction of the data to read based on uncompressed file size
(see details)}

\item{coordsonly}{Only read in XYZ coordinates of neurons.}
}
\value{
A \code{\link[nat]{neuronlist}} containing one
  \code{\link[nat]{neuron}} for each fragment
}
\description{
Read skeletons for segments by extracting from the corresponding zip file(s)

\code{read_segments2} is a reworked version of
  \code{read_segments} that reads skeletons straight from zip files to
  memory.
}
\details{
I would recommend \code{read_segments2} at this point.
  \code{read_segments} has the potential benefit of caching SWC files on disk
  rather than extracting every time. However there is a large slowdown on
  many filesystems as the number of extracted files enters the thousands -
  something that I have hit a few times. Furthermore \code{read_segments2}
  makes it easier to select fragment files \emph{before} extracting them.

  \code{datafrac} a number in the range 0-1 specifies a fraction of the data
  to read. Skeleton fragments will be placed in descending size order and
  read in until the number of bytes exceeds \code{datafrac} * sum(all file
  sizes). We have noticed that the time taken to read a neuron from a zip
  file seems to depend largely on the number of fragments that are read in,
  rather than the amount of data in each fragment! Reading 90% of the data
  can take < 10% of the time!
}
\examples{
\dontrun{
# read neuron using raw segment identifier
n <- read_segments2(22427007374)

# read a neuron from a scene specification copied from Neuroglancer window
# after clicking on the {} icon at top right
n <- read_segments2(clipr::read_clip())

summary(n)

n2 <- read_segments2(22427007374, datafrac=0.9)
summary(n2)
}
}
\seealso{
\code{\link[nat]{read.neurons}}, \code{\link{ngl_segments}},
  \code{\link{read_brainmaps_meshes}} to read 3D meshes.
}
